{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNFzKTGusAJcGxke/DhPBML",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujalkumeriya59/Deep-Learning/blob/main/practical_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Loading\n",
        "import pandas as pd\n",
        "df = pd.read_csv('diabetes.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "7IVDHJn0EZFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "# Fill missing values (e.g., with the mean)\n",
        "df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "# Normalize the features using Min-Max scaling or Standardization (e.g., StandardScaler)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# We assume the features are all columns except the 'Outcome' column (target variable)\n",
        "X = df.drop('Outcome', axis=1)\n",
        "y = df['Outcome']\n",
        "\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and test sets (80/20 split)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "8z5u6sEcFpE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Gradient Boosting\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Hypothesis function for linear regression\n",
        "def predict(X, theta):\n",
        "    return np.dot(X, theta)\n",
        "\n",
        "# Cost function (Mean Squared Error)\n",
        "def compute_cost(X, y, theta):\n",
        "    m = len(y)\n",
        "    predictions = predict(X, theta)\n",
        "    cost = (1/(2*m)) * np.sum((predictions - y) ** 2)\n",
        "    return cost\n",
        "\n",
        "# Gradient Descent function with cost print every 100 iterations\n",
        "def gradient_descent(X, y, theta, alpha, iterations):\n",
        "    m = len(y)  # number of training examples\n",
        "    cost_history = []\n",
        "\n",
        "    for i in range(iterations):\n",
        "        predictions = predict(X, theta)\n",
        "        errors = predictions - y\n",
        "\n",
        "        # Calculate gradient\n",
        "        gradients = (1/m) * np.dot(X.T, errors)\n",
        "\n",
        "        # Update parameters\n",
        "        theta = theta - alpha * gradients\n",
        "\n",
        "        # Compute and record the cost\n",
        "        cost = compute_cost(X, y, theta)\n",
        "        cost_history.append(cost)\n",
        "\n",
        "        # Print cost every 100 iterations\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f\"Iteration {i + 1}: Cost = {cost}\")\n",
        "\n",
        "    return theta, cost_history\n"
      ],
      "metadata": {
        "id": "y1U1zkEwFpnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Hypothesis function\n",
        "def predict(X, theta):\n",
        "    return np.dot(X, theta)\n",
        "\n",
        "# Cost function (Mean Squared Error)\n",
        "def compute_cost(X, y, theta):\n",
        "    m = len(y)\n",
        "    predictions = predict(X, theta)\n",
        "    cost = (1/(2*m)) * np.sum((predictions - y) ** 2)\n",
        "    return cost\n",
        "\n",
        "# Gradient descent\n",
        "def gradient_descent(X, y, theta, alpha, iterations):\n",
        "    m = len(y)\n",
        "    cost_history = []\n",
        "\n",
        "    for i in range(iterations):\n",
        "        predictions = predict(X, theta)\n",
        "        errors = predictions - y\n",
        "        gradients = (1/m) * np.dot(X.T, errors)\n",
        "        theta = theta - alpha * gradients\n",
        "\n",
        "        cost = compute_cost(X, y, theta)\n",
        "        cost_history.append(cost)\n",
        "\n",
        "        # Print cost each iteration\n",
        "        print(f\"Iteration {i+1}/{iterations} -> Cost: {cost:.6f}\")\n",
        "\n",
        "    return theta, cost_history\n",
        "\n",
        "# ---- STEP 1: PREPARE SAMPLE DATA ----\n",
        "# We'll create a simple linear dataset: y = 4 + 3x + noise\n",
        "np.random.seed(42)\n",
        "m = 100  # number of samples\n",
        "X = 2 * np.random.rand(m, 1)\n",
        "y = 4 + 3 * X + np.random.randn(m, 1)\n",
        "\n",
        "# Add intercept term (bias) to X\n",
        "X_b = np.c_[np.ones((m, 1)), X]  # shape (m, 2)\n",
        "\n",
        "# Initial parameters (theta0 and theta1)\n",
        "theta_init = np.zeros((2, 1))\n",
        "\n",
        "# ---- STEP 2: RUN GRADIENT DESCENT ----\n",
        "alpha = 0.1    # learning rate\n",
        "iterations = 100\n",
        "theta_opt, cost_history = gradient_descent(X_b, y, theta_init, alpha, iterations)\n",
        "\n",
        "print(\"\\nOptimized parameters (theta):\")\n",
        "print(theta_opt)\n",
        "\n",
        "# ---- STEP 3: PLOT COST FUNCTION ----\n",
        "plt.plot(range(1, iterations + 1), cost_history)\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Cost (MSE)\")\n",
        "plt.title(\"Cost Function vs Iterations\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HEiTNYEkKb9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z6_8AD_UKcBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FmbrVo8GKcF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fdup4g26KcJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hNED6YGgKcMu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}